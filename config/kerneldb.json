{
	// --------------------------------------------------------------------
	// Parameters that affect behavior

	// If true, the database will be created if it is missing.
	"create_if_missing": true,

	// If true, an error is raised if the database already exists.
	"error_if_exists": false,

	// If true, the implementation will do aggressive checking of the
	// data it is processing and will stop early if it detects any
	// errors.  This may have unforeseen ramifications: for example, a
	// corruption of one DB entry may cause a large number of entries to
	// become unreadable or for the entire DB to become unopenable.
	// If any of the writes to the database fails (Put, Delete, Merge, Write),
	// the database will switch to read-only mode and fail all other
	// Write operations.
	"paranoid_checks": true,


	// --------------------------------------------------------------------
	// Parameters that affect performance

	// Amount of data to build up in memory (backed by an unsorted log
	// on disk) before converting to a sorted on-disk file.
	//
	// Larger values increase performance, especially during bulk loads.
	// Up to max_write_buffer_number write buffers may be held in memory
	// at the same time,
	// so you may wish to adjust this parameter to control memory usage.
	// Also, a larger write buffer will result in a longer recovery time
	// the next time the database is opened.
	//
	// Default: 4MB
	"write_buffer_size": 4194304,

	// The maximum number of write buffers that are built up in memory.
	// The default is 2, so that when 1 write buffer is being flushed to
	// storage, new writes can continue to the other write buffer.
	"max_write_buffer_number": 2,

	// The minimum number of write buffers that will be merged together
	// before writing to storage.  If set to 1, then
	// all write buffers are fushed to L0 as individual files and this increases
	// read amplification because a get request has to check in all of these
	// files. Also, an in-memory merge may result in writing lesser
	// data to storage if there are duplicate records in each of these
	// individual write buffers.
	"min_write_buffer_number_to_merge": 1,

	// Number of open files that can be used by the DB.  You may need to
	// increase this if your database has a large working set. Value -1 means
	// files opened are always kept open. You can estimate number of files based
	// on target_file_size_base and target_file_size_multiplier for level-based
	// compaction. For universal-style compaction, you can usually set it to -1.
	"max_open_files": 5000,

	// Approximate size of user data packed per block.  Note that the
	// block size specified here corresponds to uncompressed data.  The
	// actual size of the unit read from disk may be smaller if
	// compression is enabled.  This parameter can be changed dynamically.
	//
	// Default: 4K
	"block_size": 4096,

	// Number of keys between restart points for delta encoding of keys.
	// This parameter can be changed dynamically.  Most clients should
	// leave this parameter alone.
	"block_restart_interval": 16,

	// Number of levels for this database
	"num_levels": 7,

	// If true, then the contents of data files are not synced
	// to stable storage. Their contents remain in the OS buffers till the
	// OS decides to flush them. This option is good for bulk-loading
	// of data. Once the bulk-loading is complete, please issue a
	// sync to the OS to flush all dirty buffesrs to stable storage.
	"disable_data_sync": false,

	// If true, then every store to stable storage will issue a fsync.
	// If false, then every store to stable storage will issue a fdatasync.
	// This parameter should be set to true while storing data to
	// filesystem like ext3 that can lose files after a reboot.
	"use_fsync": false,

	// Maximum number of concurrent background jobs, submitted to
	// the default LOW priority thread pool
	"max_background_compactions": 1,

	// Maximum number of concurrent background memtable flush jobs, submitted to
	// the HIGH priority thread pool.
	// By default, all background jobs (major compaction and memtable flush) go
	// to the LOW priority pool. If this option is set to a positive number,
	// memtable flush jobs will be submitted to the HIGH priority pool.
	// It is important when the same Env is shared by multiple db instances.
	// Without a separate pool, long running major compaction jobs could
	// potentially block memtable flush jobs of other db instances, leading to
	// unnecessary Put stalls.
	"max_background_flushes": 1,

	// Specify the maximal size of the info log file. If the log file
	// is larger than `max_log_file_size`, a new info log file will
	// be created.
	// If max_log_file_size == 0, all logs will be written to one
	// log file.
	"max_log_file_size": 0,

	// Time for the info log file to roll (in seconds).
	// If specified with non-zero value, log file will be rolled
	// if it has been active longer than `log_file_time_to_roll`.
	// Default: 0 (disabled)
	"log_file_time_to_roll": 0,

	// Maximal info log files to be kept.
	"keep_log_file_num": 1000
}